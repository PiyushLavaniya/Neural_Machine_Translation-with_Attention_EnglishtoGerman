{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ybNR7dNdqeiS",
      "metadata": {
        "id": "ybNR7dNdqeiS"
      },
      "source": [
        "## Importing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y0xcRn015lhL",
      "metadata": {
        "id": "y0xcRn015lhL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7vhU9CKcrJs4",
      "metadata": {
        "id": "7vhU9CKcrJs4"
      },
      "source": [
        "## Preprocessing the Data for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G5Cw1WNvqnPT",
      "metadata": {
        "id": "G5Cw1WNvqnPT"
      },
      "source": [
        "### Downloading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BzyDsjUl5lhO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzyDsjUl5lhO",
        "outputId": "52b74385-ea0f-42b9-930c-f2ceba2306c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py:743: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_stream_fn = trax.data.TFDS('opus/medical',\n",
        "                                 data_dir='./data/',\n",
        "                                 keys=('en', 'de'),\n",
        "                                 eval_holdout_size=0.01, # 1% for eval\n",
        "                                 train=True\n",
        "                                )\n",
        "\n",
        "\n",
        "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
        "                                data_dir='./data/',\n",
        "                                keys=('en', 'de'),\n",
        "                                eval_holdout_size=0.01, # 1% for eval\n",
        "                                train=False\n",
        "                               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zpJSajEW5lhQ",
      "metadata": {
        "id": "zpJSajEW5lhQ"
      },
      "outputs": [],
      "source": [
        "train_stream = train_stream_fn()\n",
        "\n",
        "eval_stream = eval_stream_fn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GoFWVPfS64od",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoFWVPfS64od",
        "outputId": "9cbd564a-88c2-4fee-c3f6-6c8b079aef3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(b'In the pregnant rat the AUC for calculated free drug at this dose was approximately 18 times the human AUC at a 20 mg dose.\\n', b'Bei tr\\xc3\\xa4chtigen Ratten war die AUC f\\xc3\\xbcr die berechnete ungebundene Substanz bei dieser Dosis etwa 18-mal h\\xc3\\xb6her als die AUC beim Menschen bei einer 20 mg Dosis.\\n')\n",
            "(b'Subcutaneous use and intravenous use.\\n', b'Subkutane Anwendung und intraven\\xc3\\xb6se Anwendung.\\n')\n"
          ]
        }
      ],
      "source": [
        "print(next(train_stream))\n",
        "print(next(eval_stream))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yVOyvJyuqq9J",
      "metadata": {
        "id": "yVOyvJyuqq9J"
      },
      "source": [
        "### Tokenizing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rQ-mnQNN5lhR",
      "metadata": {
        "id": "rQ-mnQNN5lhR"
      },
      "outputs": [],
      "source": [
        "# Tokenizing our Data\n",
        "VOCAB_FILE = 'ende_32k.subword'\n",
        "VOCAB_DIR = 'data/'\n",
        "\n",
        "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
        "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vjh4nnA-5lhS",
      "metadata": {
        "id": "Vjh4nnA-5lhS"
      },
      "outputs": [],
      "source": [
        "##Appending EOS token at the end of the Sentence\n",
        "EOS = 1\n",
        "\n",
        "def append_eos(stream):\n",
        "    for (inputs, targets) in stream:\n",
        "        inputs_with_eos = list(inputs) + [EOS]\n",
        "        targets_with_eos = list(targets) + [EOS]\n",
        "        yield np.array(inputs_with_eos), np.array(targets_with_eos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PotHVqeP7FME",
      "metadata": {
        "id": "PotHVqeP7FME"
      },
      "outputs": [],
      "source": [
        "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
        "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q8rbq9O_qx_J",
      "metadata": {
        "id": "Q8rbq9O_qx_J"
      },
      "source": [
        "### Filtering out too long Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P7pTk5hp5lhT",
      "metadata": {
        "id": "P7pTk5hp5lhT"
      },
      "outputs": [],
      "source": [
        "filtered_train_stream = trax.data.FilterByLength(max_length=512, length_keys=[0, 1])(tokenized_train_stream)\n",
        "filtered_eval_stream = trax.data.FilterByLength(max_length=512, length_keys=[0, 1])(tokenized_eval_stream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "is09cxUl7Rf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is09cxUl7Rf7",
        "outputId": "108eea10-4027-4de2-9748-dab9e0af5e44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 8569  4094  2679 32826 22527     5 30650  4729   992     1]\n",
            "[12647 19749    70 32826 10008     5 30650  4729   992     1]\n"
          ]
        }
      ],
      "source": [
        "train_input, train_target = next(filtered_train_stream)\n",
        "print(train_input)\n",
        "print(train_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JyMpiBHeq9DR",
      "metadata": {
        "id": "JyMpiBHeq9DR"
      },
      "source": [
        "### Helper Functions for Tokenizing and Detokenizing the Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9OW7HYUF7e6a",
      "metadata": {
        "id": "9OW7HYUF7e6a"
      },
      "outputs": [],
      "source": [
        "##Let's create functions to tokenize and detokenize the Sentences or Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kXGw1QNo5lhU",
      "metadata": {
        "id": "kXGw1QNo5lhU"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence, vocab_file=None, vocab_dir=None):\n",
        "\n",
        "    ##Setting the EOS token Encoding\n",
        "    EOS = 1\n",
        "\n",
        "    # we get around it by making a 1-element stream with `iter`.\n",
        "    inputs =  next(trax.data.tokenize(iter([sentence]),\n",
        "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "\n",
        "    inputs = list(inputs) + [EOS]\n",
        "\n",
        "    # Adding the batch dimension to the front of the shape\n",
        "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
        "\n",
        "    return batch_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VETinbcS7ZQT",
      "metadata": {
        "id": "VETinbcS7ZQT"
      },
      "outputs": [],
      "source": [
        "def detokenize(tokenized_sentence, vocab_file=None, vocab_dir=None):\n",
        "\n",
        "    ##Removing the dimensions of size 1\n",
        "    tokenized_sentence = list(np.squeeze(tokenized_sentence))\n",
        "\n",
        "    EOS = 1\n",
        "\n",
        "    if EOS in tokenized_sentence:\n",
        "        tokenized_sentence = tokenized_sentence[:tokenized_sentence.index(EOS)]\n",
        "\n",
        "    return trax.data.detokenize(tokenized_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-WVoFKna8HHZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WVoFKna8HHZ",
        "outputId": "9b85c5fe-2a85-4a98-86de-e0ea4f0bd3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decreased Appetite\n",
            "\n",
            "Verminderter Appetit\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##Let's test these functions\n",
        "print(detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LOtsiOf_8QYh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOtsiOf_8QYh",
        "outputId": "59b5a7cf-f4b4-4100-c221-8d5de15aec19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[17332   140     1]]\n"
          ]
        }
      ],
      "source": [
        "print(tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lSic8OQBrT_n",
      "metadata": {
        "id": "lSic8OQBrT_n"
      },
      "source": [
        "### Getting the Data into Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Icc2JYkd8PDH",
      "metadata": {
        "id": "Icc2JYkd8PDH"
      },
      "outputs": [],
      "source": [
        "##Now let's create stream of batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uY8nGkhi5lhV",
      "metadata": {
        "id": "uY8nGkhi5lhV"
      },
      "outputs": [],
      "source": [
        "boundaries =  [8,   16,  32, 64, 128, 256, 512]\n",
        "batch_sizes = [256, 128, 64, 32, 16, 8, 4, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oYiUbBkA8a-P",
      "metadata": {
        "id": "oYiUbBkA8a-P"
      },
      "outputs": [],
      "source": [
        "# Creating the generators.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes, length_keys=[0, 1])(filtered_train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes, length_keys=[0, 1])(filtered_eval_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XPobPrKjrZR_",
      "metadata": {
        "id": "XPobPrKjrZR_"
      },
      "source": [
        "### Add Padding for all the Sequences to have same length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0qSf59XH8h2G",
      "metadata": {
        "id": "0qSf59XH8h2G"
      },
      "outputs": [],
      "source": [
        "##Add padding\n",
        "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
        "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-hualpWb5lhW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hualpWb5lhW",
        "outputId": "55563051-0537-4886-fc10-7ea2f8ed3c50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_batch data type:  <class 'numpy.ndarray'>\n",
            "target_batch data type:  <class 'numpy.ndarray'>\n",
            "input_batch shape:  (32, 64)\n",
            "target_batch shape:  (32, 64)\n"
          ]
        }
      ],
      "source": [
        "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
        "\n",
        "print(\"input_batch data type: \", type(input_batch))\n",
        "print(\"target_batch data type: \", type(target_batch))\n",
        "\n",
        "print(\"input_batch shape: \", input_batch.shape)\n",
        "print(\"target_batch shape: \", target_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iILPJAk5ruS4",
      "metadata": {
        "id": "iILPJAk5ruS4"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zu5WVVzXrxRu",
      "metadata": {
        "id": "Zu5WVVzXrxRu"
      },
      "source": [
        "### Now we will create the Model for training. The Model will be consisting of an Encoder and two Decoders.\n",
        "#### The Encoder will take the Inputs and outputs the Keys and Values.\n",
        "#### The first is the Pre-Attention Decoder, it will take in targets and outputs the Queries. It will also make use of Teacher Forcing and the targets will be shifted to the right.\n",
        "#### The Keys, Values and Queries will then be passed to the Attention layer to get the Output/Context Vectors.\n",
        "#### Then these will be passed to another Decoder, that will give the final output translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TK9E0F9n9ESO",
      "metadata": {
        "id": "TK9E0F9n9ESO"
      },
      "outputs": [],
      "source": [
        "##let's implement the Attention Mechanism and create our model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Csm--4nOtWhz",
      "metadata": {
        "id": "Csm--4nOtWhz"
      },
      "source": [
        "### Defining the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y9N38-7D5lhZ",
      "metadata": {
        "id": "Y9N38-7D5lhZ"
      },
      "outputs": [],
      "source": [
        "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
        "\n",
        "    # creating a serial network\n",
        "    input_encoder = tl.Serial(\n",
        "\n",
        "        # create an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(input_vocab_size, d_model),\n",
        "\n",
        "        # feed the embeddings to the LSTM layers.\n",
        "        [tl.LSTM(n_units = d_model) for _ in range(n_encoder_layers)]\n",
        "    )\n",
        "\n",
        "    return input_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lM8dR5Fyta8q",
      "metadata": {
        "id": "lM8dR5Fyta8q"
      },
      "source": [
        "### Defining the Pre-Attention Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RIdvURHV5lha",
      "metadata": {
        "id": "RIdvURHV5lha"
      },
      "outputs": [],
      "source": [
        "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
        "\n",
        "    ##create a serial network\n",
        "    pre_attention_decoder = tl.Serial(\n",
        "\n",
        "        ##shift right to insert start-of-sentence token and implement teacher forcing during training\n",
        "        tl.ShiftRight(),\n",
        "\n",
        "        ##running an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(target_vocab_size, d_model),\n",
        "\n",
        "        ##feed it to an LSTM layer\n",
        "        tl.LSTM(n_units = d_model)\n",
        "    )\n",
        "\n",
        "    return pre_attention_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5oM86Q7Xteyg",
      "metadata": {
        "id": "5oM86Q7Xteyg"
      },
      "source": [
        "### Preparing the Inputs (Queries, Keys and Values) for the Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sYt5VtZY9xTF",
      "metadata": {
        "id": "sYt5VtZY9xTF"
      },
      "outputs": [],
      "source": [
        "##Now let's prepare inputs for the Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MxfWjgmQ5lhc",
      "metadata": {
        "id": "MxfWjgmQ5lhc"
      },
      "outputs": [],
      "source": [
        "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
        "\n",
        "    # set the keys and values to the encoder activations\n",
        "    keys = encoder_activations\n",
        "    values = encoder_activations\n",
        "\n",
        "    # set the queries to the decoder activations\n",
        "    queries = decoder_activations\n",
        "\n",
        "    # generate the mask to distinguish real tokens from padding\n",
        "    mask = (inputs != 0)\n",
        "\n",
        "    # add axes to the mask for attention heads and decoder length.\n",
        "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
        "\n",
        "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n",
        "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))  ## number of attention heads are 1.\n",
        "\n",
        "    return queries, keys, values, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xujNED_Stl4Z",
      "metadata": {
        "id": "xujNED_Stl4Z"
      },
      "source": [
        "### Creating our Model\n",
        "- Encoder and Pre-Attention Decoder will run in Parallel.\n",
        "- There will be a Residual Connection in the Attention Layer.\n",
        "- 'Select' is used for removing masks before passing the output vectors to the final Decoder for translation and also for copying inputs and targets in the beginning.\n",
        "- 'LogSoftmax' is for log Probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KoD-p_Cb5lhe",
      "metadata": {
        "id": "KoD-p_Cb5lhe"
      },
      "outputs": [],
      "source": [
        "def NMTAttn(input_vocab_size=33300,\n",
        "            target_vocab_size=33300,\n",
        "            d_model=1024,\n",
        "            n_encoder_layers=2,\n",
        "            n_decoder_layers=2,\n",
        "            n_attention_heads=4,\n",
        "            attention_dropout=0.0,\n",
        "            mode='train'):\n",
        "\n",
        "\n",
        "    #creating layers for the input encoder\n",
        "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
        "\n",
        "    #creating layers for the pre-attention decoder\n",
        "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
        "\n",
        "    model = tl.Serial(\n",
        "\n",
        "      #copy input tokens and target tokens as they will be needed later.\n",
        "      tl.Select([0, 1, 0, 1]),\n",
        "\n",
        "      #run input encoder on the input and pre-attention decoder the target in Parallel.\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
        "\n",
        "      #prepare queries, keys, values and mask for attention.\n",
        "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
        "\n",
        "      #run the AttentionQKV layer\n",
        "      #nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n",
        "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode='train')),\n",
        "\n",
        "      #drop the attention mask\n",
        "      tl.Select([0, 2]),\n",
        "\n",
        "      #run the rest of the RNN decoder\n",
        "      [tl.LSTM(n_units = d_model) for _ in range(n_decoder_layers)],\n",
        "\n",
        "      tl.Dense(target_vocab_size),\n",
        "\n",
        "      #for output\n",
        "      tl.LogSoftmax()\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ww_QrFjV5lhf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww_QrFjV5lhf",
        "outputId": "abb98810-c660-43ee-a992-ac762aeb423f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Serial_in2_out2[\n",
            "  Select[0,1,0,1]_in2_out4\n",
            "  Parallel_in2_out2[\n",
            "    Serial[\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "    Serial[\n",
            "      Serial[\n",
            "        ShiftRight(1)\n",
            "      ]\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "  ]\n",
            "  PrepareAttentionInput_in3_out4\n",
            "  Serial_in4_out2[\n",
            "    Branch_in4_out3[\n",
            "      None\n",
            "      Serial_in4_out2[\n",
            "        _in4_out4\n",
            "        Serial_in4_out2[\n",
            "          Parallel_in3_out3[\n",
            "            Dense_1024\n",
            "            Dense_1024\n",
            "            Dense_1024\n",
            "          ]\n",
            "          PureAttention_in4_out2\n",
            "          Dense_1024\n",
            "        ]\n",
            "        _in2_out2\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Select[0,2]_in3_out2\n",
            "  LSTM_1024\n",
            "  LSTM_1024\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# print your model\n",
        "model = NMTAttn()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GtmUt6Ql-pNh",
      "metadata": {
        "id": "GtmUt6Ql-pNh"
      },
      "outputs": [],
      "source": [
        "##Ok so now our model is Created."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acgk__PgucGc",
      "metadata": {
        "id": "acgk__PgucGc"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oRxMV-eM-qa7",
      "metadata": {
        "id": "oRxMV-eM-qa7"
      },
      "outputs": [],
      "source": [
        "##Let's train it"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "igCJ86EqunAt",
      "metadata": {
        "id": "igCJ86EqunAt"
      },
      "source": [
        "### Defining the Training Task and Eval Task for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jWlaADtp5lhh",
      "metadata": {
        "id": "jWlaADtp5lhh"
      },
      "outputs": [],
      "source": [
        "def train_task_function(train_batch_stream):\n",
        "\n",
        "    return training.TrainTask(\n",
        "\n",
        "        labeled_data = train_batch_stream,\n",
        "\n",
        "        loss_layer = tl.CrossEntropyLoss(),\n",
        "\n",
        "        optimizer = trax.optimizers.Adam(0.01),\n",
        "\n",
        "        lr_schedule = trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
        "\n",
        "        ##checkpoint every 10 steps\n",
        "        n_steps_per_checkpoint = 10\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "neyI9ste5lhi",
      "metadata": {
        "id": "neyI9ste5lhi"
      },
      "outputs": [],
      "source": [
        "##Instantiating the Training Task\n",
        "train_task = train_task_function(train_batch_stream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81l7WWuF5lhj",
      "metadata": {
        "id": "81l7WWuF5lhj"
      },
      "outputs": [],
      "source": [
        "##Let's also define the eval task\n",
        "eval_task = training.EvalTask(\n",
        "\n",
        "    labeled_data=eval_batch_stream,\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TPBFxO0j_D2R",
      "metadata": {
        "id": "TPBFxO0j_D2R"
      },
      "outputs": [],
      "source": [
        "##Let's start the training, for that we need to create a train loop, that is very easy using trax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xNVa5peQ5lhj",
      "metadata": {
        "id": "xNVa5peQ5lhj",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "output_dir = 'output_dir/'\n",
        "\n",
        "\n",
        "# defining the training loop\n",
        "training_loop = training.Loop(NMTAttn(mode='train'),\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hLJqjyvou12e",
      "metadata": {
        "id": "hLJqjyvou12e"
      },
      "source": [
        "### Start the Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qmwNP3KM5lhk",
      "metadata": {
        "id": "qmwNP3KM5lhk"
      },
      "outputs": [],
      "source": [
        "##Now we can just start training by writing\n",
        "training_loop.run(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yzq5vMjU_cJ3",
      "metadata": {
        "id": "Yzq5vMjU_cJ3"
      },
      "outputs": [],
      "source": [
        "##It takes a long time to run, so let's just load a pre-trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OY1594N5u8Ul",
      "metadata": {
        "id": "OY1594N5u8Ul"
      },
      "source": [
        "## Testing the Model and translating the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o_Ri6qYdEaPX",
      "metadata": {
        "id": "o_Ri6qYdEaPX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "model = os.path.join('Files', 'output_dir', 'model.pkl.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kDpiZXaE5lhl",
      "metadata": {
        "id": "kDpiZXaE5lhl"
      },
      "outputs": [],
      "source": [
        "#Instantiating the model we built in eval mode\n",
        "model = NMTAttn(mode='eval')\n",
        "\n",
        "##initializing weights from a pre-trained model\n",
        "model.init_from_file('/content/Files/output_dir/model.pkl.gz', weights_only=True)\n",
        "model = tl.Accelerate(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OodzmHc5vEsp",
      "metadata": {
        "id": "OodzmHc5vEsp"
      },
      "source": [
        "### Helper Function to get the next word in Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "freArUi85lhl",
      "metadata": {
        "id": "freArUi85lhl"
      },
      "outputs": [],
      "source": [
        "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
        "\n",
        "\n",
        "    #Getting the length of the current output tokens\n",
        "    token_length = len(cur_output_tokens)\n",
        "\n",
        "    #calculating the padding length\n",
        "    padded_length = np.power(2, int(np.ceil(np.log2(token_length + 1))))\n",
        "\n",
        "    #pad cur_output_tokens up to the padded_length\n",
        "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
        "\n",
        "    # model expects the output to have an axis for the batch size in front so\n",
        "    padded_with_batch = np.array(padded).reshape(1, padded_length)\n",
        "\n",
        "    ##model prediction\n",
        "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
        "\n",
        "    ##get log probabilities slice for the next token\n",
        "    log_probs = output[0, token_length, : ]\n",
        "\n",
        "    ##get the next symbol/word by getting a logsoftmax sample\n",
        "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
        "\n",
        "    return symbol, float(log_probs[symbol])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5EvMnt6vOfA",
      "metadata": {
        "id": "d5EvMnt6vOfA"
      },
      "source": [
        "### Helper Function to translate the whole Sentence.\n",
        "#### It will make use of the function we defined above for getting the next translated word and translate the whole sentence or sequence until the end of sentence (EOS) token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tnu_OdKz5lhn",
      "metadata": {
        "id": "Tnu_OdKz5lhn"
      },
      "outputs": [],
      "source": [
        "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
        "\n",
        "    ##Tokenizing the input sentence\n",
        "    input_tokens = tokenize(input_sentence, vocab_file, vocab_dir)\n",
        "\n",
        "    ##Initializing an empty the list of output tokens\n",
        "    cur_output_tokens = []\n",
        "\n",
        "    # initialize an integer that represents the current output\n",
        "    cur_output = 0\n",
        "\n",
        "    # Set the encoding of the \"end of sentence\"\n",
        "    EOS = 1\n",
        "\n",
        "    while cur_output != EOS:\n",
        "\n",
        "        ##updating the current output token by getting the index of the next word\n",
        "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
        "\n",
        "        ##append the current output token to the list of output tokens\n",
        "        cur_output_tokens.append(cur_output)\n",
        "\n",
        "    ##detokenize the output tokens\n",
        "    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir)\n",
        "\n",
        "    return cur_output_tokens, log_prob, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AZNnEXxR5lhn",
      "metadata": {
        "id": "AZNnEXxR5lhn"
      },
      "outputs": [],
      "source": [
        "sampling_decode(\"I love languages.\", NMTAttn=model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jDA19njRvua6",
      "metadata": {
        "id": "jDA19njRvua6"
      },
      "source": [
        "### Greedy Decode to get the Most Porbable next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rF2xCrdUGfqG",
      "metadata": {
        "id": "rF2xCrdUGfqG"
      },
      "outputs": [],
      "source": [
        "##Let's use the Greedy Decode Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AmC7dMvx5lho",
      "metadata": {
        "id": "AmC7dMvx5lho"
      },
      "outputs": [],
      "source": [
        "def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
        "\n",
        "    _,_, translated_sentence = sampling_decode(sentence, NMTAttn=NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize)\n",
        "\n",
        "    print(\"English: \", sentence)\n",
        "    print(\"German: \", translated_sentence)\n",
        "\n",
        "    return translated_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VGq1wnet5lhp",
      "metadata": {
        "id": "VGq1wnet5lhp",
        "outputId": "359d41dc-8c90-4aa8-dadf-e429c338d18d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English:  My name is Piyush.\n",
            "German:  Ich heiße Piyush.\n"
          ]
        }
      ],
      "source": [
        "#put a custom string here\n",
        "your_sentence = 'My name is Piyush.'\n",
        "\n",
        "greedy_decode_test(your_sentence, NMTAttn=model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I1GmzhLE5lhp",
      "metadata": {
        "id": "I1GmzhLE5lhp",
        "outputId": "7dba3054-c0e3-44d6-cc83-076f73a00ec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English:  You are almost done with the assignment!\n",
            "German:  Sie sind fast mit der Aufgabe fertig!\n"
          ]
        }
      ],
      "source": [
        "greedy_decode_test('You are almost done with the assignment!', model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M-AKmz525lhq",
      "metadata": {
        "id": "M-AKmz525lhq"
      },
      "source": [
        "<a name=\"4-2\"></a>\n",
        "### Implement Minimum Bayes-Risk Decoding\n",
        "\n",
        "Now getting the most probable token at each step may not necessarily produce the best results.\n",
        "Another approach and the more efficient one is to do Minimum Bayes Risk Decoding or MBR.\n",
        "We can implement it like this:\n",
        "- take several random samples (samples are our translated sequences)\n",
        "- score each sample against all other samples\n",
        "- select the one with the highest score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gyAbkdnRHOND",
      "metadata": {
        "id": "gyAbkdnRHOND"
      },
      "outputs": [],
      "source": [
        "##Now let's use Minimum Bayes Risk to decode our Sample or translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D4S55CaOHW5k",
      "metadata": {
        "id": "D4S55CaOHW5k"
      },
      "outputs": [],
      "source": [
        "##let's create a function to generate samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "--0hRfsjyMbb",
      "metadata": {
        "id": "--0hRfsjyMbb"
      },
      "source": [
        "### Helper Function for Generating Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FYrVh35i5lhr",
      "metadata": {
        "id": "FYrVh35i5lhr"
      },
      "outputs": [],
      "source": [
        "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
        "\n",
        "    #lists to contain samples and probabilities\n",
        "    samples, log_probs = [], []\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "\n",
        "        # get a sample using the sampling_decode() function\n",
        "        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir, next_symbol=next_symbol)\n",
        "\n",
        "        # append the token list to the samples list\n",
        "        samples.append(sample)\n",
        "\n",
        "        # append the log probability to the log_probs list\n",
        "        log_probs.append(logp)\n",
        "\n",
        "    return samples, log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RGLrVCWM5lhr",
      "metadata": {
        "id": "RGLrVCWM5lhr",
        "outputId": "a44959e9-bfdc-4868-c9cb-d6ea07514958"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([[7906, 644, 21, 352, 102, 1],\n",
              "  [595, 75, 67, 352, 102, 1],\n",
              "  [595, 119, 67, 352, 102, 1],\n",
              "  [595, 24, 34, 352, 102, 1]],\n",
              " [-0.0004901885986328125,\n",
              "  -3.814697265625e-06,\n",
              "  -1.33514404296875e-05,\n",
              "  -7.62939453125e-06])"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_samples('how are you today?', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gs_bKV3J5lhs",
      "metadata": {
        "id": "gs_bKV3J5lhs"
      },
      "source": [
        "<a name='4-2-2'></a>\n",
        "### Helper Function to Compare the Overlaps\n",
        "\n",
        "Let us now build our functions to compare a sample against another. We will be calculating scores for unigram overlaps. One of the more simple metrics is the *Jaccard* similarity which gets the intersection over union of two sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J_ux4_d1Hqkx",
      "metadata": {
        "id": "J_ux4_d1Hqkx"
      },
      "outputs": [],
      "source": [
        "##Now let's compare the generated samples against eachother."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GoqRKQJPyH9L",
      "metadata": {
        "id": "GoqRKQJPyH9L"
      },
      "source": [
        "### Implementing Jaccard Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B9IsGuqI5lhs",
      "metadata": {
        "id": "B9IsGuqI5lhs"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "\n",
        "    #convert the lists to a set to get the unique tokens\n",
        "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)\n",
        "\n",
        "    #set of tokens common to both candidate and reference\n",
        "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
        "\n",
        "    #set of all tokens found in either candidate or reference\n",
        "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
        "\n",
        "    #divide the number of joint elements by the number of all elements\n",
        "    overlap = len(joint_elems) / len(all_elems)\n",
        "\n",
        "    return overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AMlyOFV-5lhs",
      "metadata": {
        "id": "AMlyOFV-5lhs",
        "outputId": "8959f421-d4bc-4854-d6bf-c0fe0328c597"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's try using the function. remember the result here and compare with the next function below.\n",
        "jaccard_similarity([1, 2, 3], [1, 2, 3, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u0wGgSUEyaVN",
      "metadata": {
        "id": "u0wGgSUEyaVN"
      },
      "source": [
        "### Calculating the Overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6VWCT9xf5lhv",
      "metadata": {
        "id": "6VWCT9xf5lhv"
      },
      "outputs": [],
      "source": [
        "def average_overlap(similarity_fn, samples, *ignore_params):\n",
        "    ##Returns the arithmetic mean of each candidate sentence in the samples\n",
        "\n",
        "    # initialize dictionary\n",
        "    scores = {}\n",
        "\n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):\n",
        "\n",
        "        # initialize overlap\n",
        "        overlap = 0.0\n",
        "\n",
        "        # run a for loop for each sample\n",
        "        for index_sample, sample in enumerate(samples):\n",
        "\n",
        "            # skip if the candidate index is the same as the sample index\n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "\n",
        "            # get the overlap between candidate and sample using the similarity function\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "\n",
        "            # add the sample overlap to the total overlap\n",
        "            overlap += sample_overlap\n",
        "\n",
        "        # get the score for the candidate by computing the average\n",
        "        score = overlap / index_sample\n",
        "\n",
        "        # save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dx19Szwfwk8I",
      "metadata": {
        "id": "Dx19Szwfwk8I"
      },
      "source": [
        "### Using all the Functions above and Implementing MBR Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QD6NTj9D5lhz",
      "metadata": {
        "id": "QD6NTj9D5lhz"
      },
      "outputs": [],
      "source": [
        "def mbr_decode(sentence, n_samples, average_overlap=average_overlap, jaccard_similarity=jaccard_similarity, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None, generate_samples=generate_samples, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
        "\n",
        "    #generating the samples\n",
        "    samples, log_probs = generate_samples(sentence, n_samples, NMTAttn, temperature, vocab_file, vocab_dir)\n",
        "\n",
        "    # use the scoring function to get a dictionary of scores\n",
        "    scores = average_overlap(jaccard_similarity, samples, log_probs)\n",
        "\n",
        "    # find the key with the highest score\n",
        "    max_score_key = max(scores, key = scores.get)\n",
        "\n",
        "    # detokenize the token list associated with the max_score_key\n",
        "    translated_sentence = detokenize(samples[max_score_key], vocab_file, vocab_dir)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return (translated_sentence, max_score_key, scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gND1FozF5lh0",
      "metadata": {
        "id": "gND1FozF5lh0"
      },
      "outputs": [],
      "source": [
        "TEMPERATURE = 1.0\n",
        "\n",
        "# put a custom string here\n",
        "your_sentence = 'She speaks English and German.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V2WCnqVM5lh0",
      "metadata": {
        "id": "V2WCnqVM5lh0",
        "outputId": "348882ae-cbe8-4e65-fd02-a154131b5e23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Sie spricht Englisch und Deutsch.'"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mbr_decode(your_sentence, 4, average_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
